{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T06:47:23.876758Z",
          "iopub.execute_input": "2025-09-17T06:47:23.877071Z",
          "iopub.status.idle": "2025-09-17T06:47:23.890789Z",
          "shell.execute_reply.started": "2025-09-17T06:47:23.877040Z",
          "shell.execute_reply": "2025-09-17T06:47:23.887926Z"
        },
        "id": "It3-4Qw_cBQ6"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning with TensorFlow: Feature Extraction**\n",
        "\n",
        "To improve models we can use several tenchniques like\n",
        "- Adding More Layers\n",
        "- Changing the Learning Rate\n",
        "- Adjusting the Number of Neurons Per Layer\n",
        "\n",
        "However, instead of above we can use **Transfer Learning**.\n",
        "- **Transfer Learning** is taking the patterns (also called weights) from another model and applying it on new problem.\n",
        "\n",
        "Two main benefits of using Transfer Learning.\n",
        "- Leverage existing Neural Network Architecture proven to work on problems similar to our own\n",
        "- Using **Already Learned Patterns** on similar data to our own\n",
        "\n",
        "So, instead of hand-crafting our own **Neural Network Architecture or building them from scratch** we can utilize models which have worked for others.\n",
        "\n",
        "We can take the patterns a model has learned from datasets such as **ImageNet** and use it as a foundational model.\n",
        "\n",
        "**What we will Learn**\n",
        "- Use a smaller dataset to experiment faster (10% of training samples of 10 classes of food)\n",
        "- Build a Transfer Learning Feature Extraction model using **TensorFlow Hub**\n",
        "- Introduce a TesnorBoard Callback to track model training results\n",
        "\n",
        "**Transfer Learning with TensorFlow Hub: Getting great results with only 10% of data**\n",
        "\n",
        "- **TensorFlow Hub:-** is a repository for existing model components. You can import and use a **Fully Trained Model** using a *URL*\n",
        "\n",
        "Using the **Pre-trained Models** we can get the results of a fully trained model with only 10% of data.\n",
        "\n",
        "**Transfer Learning often allows you to get great results with less data**\n",
        "\n",
        "\n",
        "Let's download 10% of training data from **10_food_classes** dataset and use it to train a food image classifier on it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n919sal9cUXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VyldLoxcStk",
        "outputId": "b9a2610d-3493-4672-9d7b-8374fd1d2bb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-23 09:30:33--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 74.125.137.207, 142.250.101.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   181MB/s    in 0.9s    \n",
            "\n",
            "2025-09-23 09:30:34 (181 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "BB5IpZFIleoc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hccRg2Vledy",
        "outputId": "9e5c2b6d-df90-4d4d-e5ac-fdd90aeadbf1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Data Loaders (Preparing the Data)**\n",
        "\n",
        "- Create the **ImageDataGenerator** class using the **flow_from_directory** method to load in our images."
      ],
      "metadata": {
        "id": "2vwq043HneRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "print(\"Training Images\")\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                                          target_size=IMAGE_SHAPE,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode=\"categorical\"\n",
        "                                                          )\n",
        "\n",
        "\n",
        "print(\"Testing Images\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=IMAGE_SHAPE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             class_mode=\"categorical\"\n",
        "                                             )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHc9cw-8leaS",
        "outputId": "159fac75-4246-4581-b164-292ca0918187"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Images\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing Images\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Callbacks (Things to Run While our Model Trains)**\n",
        "\n",
        "**Callbacks** are extra functionality that you can add to your models to be performed during or after training. Some of the most important callbacks are\n",
        "\n",
        "- **Experiment Tracking with TensorBoard:-** Log the performance of multiple models and then view and compare these models in a visual way on **TensorBoard**. **TensorBoard** is a dashboard for inspecting **Neural Network Parameters**\n",
        "\n",
        "- **Model CheckPointing:-** Save your model as you train so that you can stop training if needed and continue off where you left. It is helpful if training takes a long time and cannot be done in one sitting.\n",
        "\n",
        "- **Early Stopping:-** Leave your model training for a arbitary amount of time and have it stop training automatically when it ceases to improve. It is helpful when you have a large dataset and do not know how long training will take.\n",
        "\n",
        "- The TensorBoard Callback can be accessed using\n",
        "**tf.keras.callbacks.TensorBoard()**. - The main function of this is saving model's training performance metrics to a specified **log_dir**.\n",
        "\n",
        "- By default, logs are recorded every epoch using the **update_freq='epoch'** parameter. This is a good default but can slow down **Model Training.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wc7otuv_pQRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tensorboard callback\n",
        "\n",
        "import datetime\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n"
      ],
      "metadata": {
        "id": "lt9_L9wHleQ7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will save the **Model** to a directory [dir_name] / [experiment_name] / [current_timestamp] where\n",
        "- **dir_name:-** is the overall logs directory\n",
        "- **experiment_name:-** is the particular experiment\n",
        "- **current_timestamp:-** is the time the experiment started based on Python time *datatime.datetime().now()"
      ],
      "metadata": {
        "id": "M1wTBkSs2VWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Models Using TensorFlow Hub**\n",
        "\n",
        "- In past we used to create models from scratch\n",
        "- In here, majority of our model's layers are going to come from **TensorFlow Hub**\n",
        "\n",
        "We will use two models from **TensorFlow Hub**\n",
        "\n",
        "- 1.**ResNetV2** - a state of the art computer vision model architecture from 2016\n",
        "- 2.**EfficientNet**- a state of the art computer vision model from 2019\n",
        "\n",
        "By the state of art we mean that majority of our modeles have achieved the lowest error rate on **ImageNet (ILSVRC-2012-CLS)**, the gold standard of computer vision benchmarks.\n",
        "\n",
        "Steps for finding models on **TensorFlow Hub**\n",
        "1. Got to **tfhub.dev**\n",
        "2. Select the problem domain like **Image**\n",
        "3. Remove all **Problem Domain** filters except for the one you are working on\n",
        "4. You will see a list of models, select the one you want to use\n",
        "\n",
        "**I see many models, then which one is to be used**\n",
        "\n",
        "- You can find a list of state of the art models on **paperswithcode.com**, a resource for collecting the latest in deep learning paper results\n",
        "\n",
        "- Since, our target is **Image Classification** so we would use the model that performed best on **ImageNet.**\n",
        "\n",
        "- On **tfhub.dev** you will find various architectures like **EfficientNetB4** which is better than **EfficientNetB0** but larger models take alot of time to compute.\n"
      ],
      "metadata": {
        "id": "zWUKJMzLZ58L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use feature vectors URLs of two common computer vision architectures, **EfficientNetBO (2019)** and **ResNetV250 (2016)**\n",
        "\n",
        "Why we select only **Feature Vectors**.\n",
        "Because Transfer Learning come into play as **Feature Extraction** and **Fine Tuning**\n",
        "\n",
        "1. In **transfer Learning** we take a pre-trained models as it is and apply it to our task without changes. For example, if your model is trained on **ImageNet** dataset that contains **1000** different classes of images. So, if we pass a single image to this model it will produce **1000** different outputs. It can be useful if we want to classify **1000** images.\n",
        "\n",
        "2. **Feature Extraction Transfer Learning** is a process where you take the underlying patterns (also called weights) a **pretrained Model** has learned and adjust its output to be more suited to your problem.\n",
        "For example, If your model had 236 different layers (EfficientNetBO has 236 layers) and the top layer outputs **1000** classes because it was pretrained on **ImageNet.**To adjust it to your problem we might remove the top layer and replace it with our own having the right number of classes. The most important part here is that **only the top few layers become trainable, the rest remain frozen.**So, the underlying patterns remain in the rest of layers and we can utilize it for our problem.\n",
        "\n",
        "3. **Fine-Tuning Transfer Learning:-** is when you take the underlying patterns (also called weights) of a pre-trained model and adjust them to your problem. This means **training some, many or all layers** in pretrained model. This is applicable in scenarios where you have relatively large dataset and **your data is slightly different** from the original data on which the model was trained.\n",
        "\n",
        "\n",
        "A common practice is to **Freeze** all the learned patterns in bottom layers of a **pretrained model** so that they become **un-trainable**. Then, the top 2-3 layers of the **pre-trained** model can adjust its output to our customer data (**feature extraction**).\n",
        "As you have trained the **top layers** you can gradually **unfreeze** more and more layers and run the training process on your own data to further **fine-tune**it.\n",
        "\n",
        "- **Lower Layers** in a computer vision model learns **large features**. In a cat and dog classification they might learn the **outline of legs** while the layers closer to the output might learn **shape of the teeth.**\n",
        "\n",
        "\n",
        "So, in **Feature Extraction** only the top 2-3 layers change but in **Fine Tuning Model** many or all of the original model get changed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nc2Q_27koE2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "cfuo9Vl8leIA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet 50 V2 feature vector\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Original: EfficientNetB0 feature vector (version 1)\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "\n",
        "# # New: EfficientNetB0 feature vector (version 2)\n",
        "# efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n"
      ],
      "metadata": {
        "id": "98_y2fW9ld_Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **URLs** are link to a saved **pretrained models** on **TensorFlow Hub**.\n",
        "- When we use it in our model, the model will automatically downloaded for us to use\n",
        "- We will use **KerasLayer()** inside the TensorFlow Hub library."
      ],
      "metadata": {
        "id": "Vpry_f-YzbJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The function below helps in creating **Model**.\n",
        "- Our first model will be **ResNetV250** architecture as our feature extraction layer\n",
        "- Once our model is instantiated, we will compile it using **categorical_crossentropy** as our loss function, **Adam Optimizer** and **Accuracy** as metric."
      ],
      "metadata": {
        "id": "FY2i_Kc31LC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "NUM_CLASSES = 10  # Replace with the actual number of classes in your dataset\n",
        "RESNET_URL = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Custom layer to wrap hub.KerasLayer\n",
        "class HubLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_url, **kwargs):\n",
        "        super(HubLayer, self).__init__(**kwargs)\n",
        "        self.hub_layer = hub.KerasLayer(model_url, trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.hub_layer(inputs)\n",
        "\n",
        "def create_model_tf_hub_fixed(model_url, num_classes=NUM_CLASSES):\n",
        "    # Define input layer\n",
        "    inputs = keras.Input(shape=IMAGE_SHAPE + (3,))\n",
        "\n",
        "    # Apply custom Hub layer\n",
        "    x = HubLayer(model_url)(inputs)\n",
        "\n",
        "    # Add dropout and dense layers\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Clear Keras session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Disable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "# Create and compile the model\n",
        "resnet_model = create_model_tf_hub_fixed(RESNET_URL, num_classes=NUM_CLASSES)\n",
        "resnet_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "resnet_model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XOZekZ2woAIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "3e7f90c6-107e-4f8f-e6cb-14ee51e07563"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hub_layer (\u001b[38;5;33mHubLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m20,490\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hub_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">HubLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,490\u001b[0m (80.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> (80.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,490\u001b[0m (80.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> (80.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding the CallBack**\n",
        "- A tensorflow **CallBack** adds extra functionality by virtue of which we can track the performance of our model on TensorBoard\n",
        "- The **CallBack** is added in the **fit** method"
      ],
      "metadata": {
        "id": "5xZTf5MbhZDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_history = resnet_model.fit(\n",
        "    train_data_10_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data),\n",
        "    callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                           experiment_name=\"resnet50V2\")]\n",
        ")\n"
      ],
      "metadata": {
        "id": "XSdlwwNMoAE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning with TensorFlow Part 2: Fine-Tuning**\n",
        "\n",
        "- In the section above we saw how could we leverage **Feature Extraction Transfer Learning** to get far better results than building our own models\n",
        "- In **Fine Tuning Transfer Learning** the pre-trained weights from another model are **unfrozen and tweaked** during training to better suit your own data\n",
        "- In the **Feature Extraction Transfer Learning** we train the top **1-3 layers** of a pre-trained model with your own data\n",
        "- In the **Fine-tuning transfer learning** we might train 1 to 3 or more layers of a pre-trained model\n",
        "\n",
        "**Main Difference between Feature Extraction Transfer Learning and Fine Tuning Transfer Learning**\n",
        "- **Feature Extraction Transfer Learning:** You only train only top layers\n",
        "- **Fine Tuning Transfer Learning:** Not only top layers but some other layers beneath can be unfrozen for training."
      ],
      "metadata": {
        "id": "nCLAVDjoq9D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Path Ahead**\n",
        "\n",
        "- Introducing Fine-Tuning: A type of transfer learning to modify a pre-trained model to more suited to your data\n",
        "- Using **Keras Functional API**, a different way to build models in keras\n",
        "- Using smaller dataset to experiment faster\n",
        "- **Data Augmentation:** How to making training diverse without adding any new data"
      ],
      "metadata": {
        "id": "No_HgZd_tV60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "bdlS7A9K_Gtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### We create a bunch of helpful functions throughout the course.\n",
        "### Storing them here so they're easily accessible.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): string filename of target image\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode it into a tensor\n",
        "  img = tf.image.decode_jpeg(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img\n",
        "\n",
        "# Note: The following confusion matrix code is a remix of Scikit-Learn's\n",
        "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
        "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "\n",
        "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "  will be used.\n",
        "\n",
        "  Args:\n",
        "    y_true: Array of truth labels (must be same shape as y_pred).\n",
        "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "    figsize: Size of output figure (default=(10, 10)).\n",
        "    text_size: Size of output figure text (default=15).\n",
        "    norm: normalize values or not (default=False).\n",
        "    savefig: save confusion matrix to file (default=False).\n",
        "\n",
        "  Returns:\n",
        "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "\n",
        "  Example usage:\n",
        "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                          y_pred=y_preds, # predicted labels\n",
        "                          classes=class_names, # array of class label names\n",
        "                          figsize=(15, 15),\n",
        "                          text_size=10)\n",
        "  \"\"\"\n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "\n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes),\n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "\n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    if norm:\n",
        "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "    else:\n",
        "      plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "\n",
        "  # Save the figure to the current working directory\n",
        "  if savefig:\n",
        "    fig.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "# Make a function to predict on images and plot them (works with multi-class)\n",
        "def pred_and_plot(model, filename, class_names):\n",
        "  \"\"\"\n",
        "  Imports an image located at filename, makes a prediction on it with\n",
        "  a trained model and plots the image with the predicted class as the title.\n",
        "  \"\"\"\n",
        "  # Import the target image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # Make a prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the predicted class\n",
        "  if len(pred[0]) > 1: # check for multi-class\n",
        "    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n",
        "  else:\n",
        "    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n",
        "\n",
        "  # Plot the image and predicted class\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False);\n",
        "\n",
        "import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instand to store log files.\n",
        "\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "# Plot the validation and training data separately\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "\n",
        "  Args:\n",
        "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "  \"\"\"\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();\n",
        "\n",
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "    \"\"\"\n",
        "    Compares two TensorFlow model History objects.\n",
        "\n",
        "    Args:\n",
        "      original_history: History object from original model (before new_history)\n",
        "      new_history: History object from continued model training (after original_history)\n",
        "      initial_epochs: Number of epochs in original_history (new_history plot starts from here)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get original history measurements\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    # Make plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "# Create function to unzip a zipfile into current working directory\n",
        "# (since we're going to be downloading and unzipping a few files)\n",
        "import zipfile\n",
        "\n",
        "def unzip_data(filename):\n",
        "  \"\"\"\n",
        "  Unzips filename into the current working directory.\n",
        "\n",
        "  Args:\n",
        "    filename (str): a filepath to a target zip folder to be unzipped.\n",
        "  \"\"\"\n",
        "  zip_ref = zipfile.ZipFile(filename, \"r\")\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()\n",
        "\n",
        "# Walk through an image classification directory and find out how many files (images)\n",
        "# are in each subdirectory.\n",
        "import os\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "\n",
        "  Args:\n",
        "    dir_path (str): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "      y_true: true labels in the form of a 1D array\n",
        "      y_pred: predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results\n",
        "\n"
      ],
      "metadata": {
        "id": "Wignj0fUn_9M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10 Food Classes: Working with Less Data**\n",
        "\n",
        "- Below we will use the **in-built pretrained models** within the **tf.keras.applications** module as well as we will **fine-tune** them to our customer dataset.\n",
        "- We will be using a new but similar dataloader function to what we have used before called **image_dataset_from_directory()** which is part of **tf.keras.utils** module.\n",
        "- Finally, we will practice **Keras Functional API** for building deep learning models.\n",
        "- The **Functional API** is a more flexible way to create models than **tf.keras.Sequential API.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxkarKmf_Z04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the Data**"
      ],
      "metadata": {
        "id": "3vzUtCqSBrWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of the data of the 10 classes\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXSAxVg3q8m2",
        "outputId": "39e38bfc-70b0-43dd-c32d-7f707563ebf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-23 10:01:45--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 142.250.141.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   106MB/s    in 1.5s    \n",
            "\n",
            "2025-09-23 10:01:46 (106 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Walk through 10 percent data directory and list number of files\n",
        "walk_through_dir(\"10_food_classes_10_percent\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5b-DScbq8fe",
        "outputId": "23bfe29b-69ba-4d73-8f81-41e1abd07ca4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **training data** has **75 images** while the testing directories have **250 images.**"
      ],
      "metadata": {
        "id": "ifNeR_tiCLtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test directories\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "KYdIoEFvq8cg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Previously we used the **ImageDataGenerator** class but since **August 2023** it has been deprecated and is not recommended for future usage because of its **slowness.**\n",
        "\n",
        "- We will move onto using **tf.keras.utils.image_dataset_from_directory()**\n",
        "\n",
        "- One main benefit of using **tf.keras.preprocessing.image_dataset_from_directory()** rather than **ImageDataGenerator** is that it creates a **tf.data.Dataset** object rather than a generator\n",
        "\n",
        "- The **tf.data.Dataset** API is much more efficient(faster) than the **ImageDataGenerator** API which is paramount for larger datasets"
      ],
      "metadata": {
        "id": "IsVip88rCa_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224) # define image size\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                           image_size=IMG_SIZE,\n",
        "                                                                           label_mode=\"categorical\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv6dN7SZq8ZM",
        "outputId": "79dbacfc-39c0-45c0-cc81-e7c3021c2e68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Components of **image_dataset_from_directory()** function\n",
        "\n",
        "- **directory:-** the filepath of the target directory we are loading images in from\n",
        "- **image_size:-** target size\n",
        "- **batch_size:-** batch size we are going to load in\n",
        "- **label_mode:-** If only two classes then **binary** otherwise for more than two classes we use **categorical**"
      ],
      "metadata": {
        "id": "Peavu79HEJ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the training data datatype\n",
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNMUnqT-q8R4",
        "outputId": "481aacc3-76e2-4b8e-aa15-7979f283b772"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train_data_10_percent** attributes\n",
        "\n",
        "- **(TensorSpec(shape=(None, 224, 224, 3)** refers to the tensor shape of our images. *None* is the batch size. 224 is height and 224 is the width. **3** is the color channels (red, green, blue)\n",
        "\n",
        "- **(None, 10):-** refers to the tensor shape of the labels. **None** is the batch size and **10** is the number of possible labels."
      ],
      "metadata": {
        "id": "X0AmbZ79F5P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the class names of our dataset\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGKOLWqGq8FL",
        "outputId": "4b8f4404-a3c6-471a-a00a-1a2c0026b935"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# See an example batch of data\n",
        "for images, labels in train_data_10_percent.take(1):\n",
        "  print(images, labels)"
      ],
      "metadata": {
        "id": "NruIdE8uBjdG",
        "outputId": "55635749-c142-4bbb-a00b-3be8f34d02fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[102.015305   34.015305   33.015305 ]\n",
            "   [107.4949     39.4949     38.4949   ]\n",
            "   [107.0102     39.010204   38.010204 ]\n",
            "   ...\n",
            "   [ 26.857187    9.857186   15.857186 ]\n",
            "   [ 29.214273   12.2142725  18.214273 ]\n",
            "   [ 24.928501    7.9285016  13.928502 ]]\n",
            "\n",
            "  [[103.35714    35.35714    34.35714  ]\n",
            "   [102.352036   34.35204    33.35204  ]\n",
            "   [104.27041    36.27041    35.27041  ]\n",
            "   ...\n",
            "   [ 27.84186    10.84186    16.84186  ]\n",
            "   [ 29.071415   12.071415   18.071415 ]\n",
            "   [ 28.28064    11.28064    17.28064  ]]\n",
            "\n",
            "  [[104.62755    37.270405   35.62755  ]\n",
            "   [102.42857    35.071426   33.42857  ]\n",
            "   [105.811226   37.811226   36.47449  ]\n",
            "   ...\n",
            "   [ 27.7398     11.168372   16.954086 ]\n",
            "   [ 25.112265    8.540837   14.326551 ]\n",
            "   [ 31.428677   14.857247   20.642962 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[220.10754   212.89838   115.82716  ]\n",
            "   [205.796     200.31136    98.939064 ]\n",
            "   [193.10213   188.81134    80.097206 ]\n",
            "   ...\n",
            "   [253.35721   246.35721   228.3113   ]\n",
            "   [252.87248   249.98476   230.92862  ]\n",
            "   [255.        254.72453   235.21944  ]]\n",
            "\n",
            "  [[216.9595    213.03094   152.08221  ]\n",
            "   [213.31137   209.31139   139.8218   ]\n",
            "   [203.89801   201.62764   118.09716  ]\n",
            "   ...\n",
            "   [254.65816   249.37251   230.57147  ]\n",
            "   [254.07144   253.07144   233.07144  ]\n",
            "   [254.95407   255.        238.31125  ]]\n",
            "\n",
            "  [[254.08168   253.20927   229.97998  ]\n",
            "   [254.66833   254.0204    217.40834  ]\n",
            "   [240.10779   240.10779   180.38863  ]\n",
            "   ...\n",
            "   [253.13774   249.70921   230.70921  ]\n",
            "   [253.52545   252.66833   234.66833  ]\n",
            "   [254.35718   255.        237.35718  ]]]\n",
            "\n",
            "\n",
            " [[[ 72.841835   16.19898    25.198978 ]\n",
            "   [ 74.88776    16.09694    25.693878 ]\n",
            "   [ 75.64286    14.214286   22.35204  ]\n",
            "   ...\n",
            "   [ 83.214264   17.785736   26.       ]\n",
            "   [ 79.14288    17.357208   22.285767 ]\n",
            "   [ 88.42867    30.50021    34.143032 ]]\n",
            "\n",
            "  [[ 72.95409    12.954081   24.95408  ]\n",
            "   [ 73.21429    13.214285   23.357141 ]\n",
            "   [ 73.484695   13.413265   21.413265 ]\n",
            "   ...\n",
            "   [ 84.15814    18.729612   26.943876 ]\n",
            "   [ 79.13778    17.219427   22.214325 ]\n",
            "   [ 85.811295   27.88283    31.525652 ]]\n",
            "\n",
            "  [[ 68.4898     17.41837    22.479593 ]\n",
            "   [ 72.36735    25.36735    26.811226 ]\n",
            "   [ 70.44898    27.234697   26.54082  ]\n",
            "   ...\n",
            "   [ 83.44897    18.785713   26.785713 ]\n",
            "   [ 78.14288    17.142883   24.142883 ]\n",
            "   [ 85.42871    26.42871    30.42871  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 77.214264   19.785736   25.785736 ]\n",
            "   [ 78.41322    20.984695   26.984695 ]\n",
            "   [ 78.42853    21.         27.       ]\n",
            "   ...\n",
            "   [ 74.214264   17.214264   23.214264 ]\n",
            "   [ 74.         17.         23.       ]\n",
            "   [ 75.         18.         24.       ]]\n",
            "\n",
            "  [[ 76.         19.         25.       ]\n",
            "   [ 75.93366    18.93366    24.93366  ]\n",
            "   [ 75.92856    18.928558   24.928558 ]\n",
            "   ...\n",
            "   [ 73.94386    16.943865   22.943865 ]\n",
            "   [ 73.06634    16.066338   22.066338 ]\n",
            "   [ 74.         17.         23.       ]]\n",
            "\n",
            "  [[ 77.35718    20.357178   27.357178 ]\n",
            "   [ 75.428604   18.428606   25.428606 ]\n",
            "   [ 75.35718    18.357178   25.357178 ]\n",
            "   ...\n",
            "   [ 73.214264   16.214264   22.214264 ]\n",
            "   [ 73.         16.         22.       ]\n",
            "   [ 74.         17.         23.       ]]]\n",
            "\n",
            "\n",
            " [[[178.        172.        158.       ]\n",
            "   [178.        172.        158.       ]\n",
            "   [178.        172.        158.       ]\n",
            "   ...\n",
            "   [  2.          4.          1.       ]\n",
            "   [  2.          4.          1.       ]\n",
            "   [  3.          5.          2.       ]]\n",
            "\n",
            "  [[178.        172.        156.       ]\n",
            "   [178.        172.        156.       ]\n",
            "   [178.        172.        156.       ]\n",
            "   ...\n",
            "   [  2.          4.          1.       ]\n",
            "   [  2.9336746   4.933675    1.9336746]\n",
            "   [  3.          5.          2.       ]]\n",
            "\n",
            "  [[178.        172.        156.       ]\n",
            "   [178.        172.        156.       ]\n",
            "   [178.        172.        156.       ]\n",
            "   ...\n",
            "   [  3.          5.          2.       ]\n",
            "   [  3.0153089   5.015309    2.0153089]\n",
            "   [  4.          6.          3.       ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[218.64285   207.64285   189.64285  ]\n",
            "   [217.14285   206.14285   187.71432  ]\n",
            "   [220.42854   209.42854   190.66328  ]\n",
            "   ...\n",
            "   [165.26018   140.97429   114.61707  ]\n",
            "   [154.98457   121.99994    91.811134 ]\n",
            "   [111.1363     86.56501    64.279366 ]]\n",
            "\n",
            "  [[219.4286    208.4286    190.4286   ]\n",
            "   [219.21431   208.21431   190.21431  ]\n",
            "   [220.14288   209.14288   191.14288  ]\n",
            "   ...\n",
            "   [164.44383   138.94368   112.57117  ]\n",
            "   [156.21414   123.42847    92.64279  ]\n",
            "   [119.64668    95.07539    72.78975  ]]\n",
            "\n",
            "  [[220.34178   208.34178   192.34178  ]\n",
            "   [220.35707   209.35707   191.35707  ]\n",
            "   [221.07137   210.07137   192.07137  ]\n",
            "   ...\n",
            "   [168.21938   142.71922   116.14776  ]\n",
            "   [157.1427    124.357025   93.57135  ]\n",
            "   [124.92749   100.3562     78.07056  ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[183.35715   165.35715   153.35715  ]\n",
            "   [184.26021   166.26021   156.26021  ]\n",
            "   [187.78572   169.78572   159.78572  ]\n",
            "   ...\n",
            "   [107.13796    87.28091    80.34726  ]\n",
            "   [125.091896  114.285805  108.99499  ]\n",
            "   [132.58672   125.65826   122.30108  ]]\n",
            "\n",
            "  [[183.35715   165.35715   153.35715  ]\n",
            "   [183.92857   165.92857   153.92857  ]\n",
            "   [185.32652   167.32652   157.32652  ]\n",
            "   ...\n",
            "   [ 95.31134    74.86751    67.056366 ]\n",
            "   [109.13274    96.28583    90.29093  ]\n",
            "   [123.78585   115.18895   110.99513  ]]\n",
            "\n",
            "  [[183.        165.        153.       ]\n",
            "   [182.80103   164.80103   152.80103  ]\n",
            "   [182.2143    164.2143    152.55103  ]\n",
            "   ...\n",
            "   [ 86.59191    63.37765    54.949123 ]\n",
            "   [ 99.57155    82.28586    75.398125 ]\n",
            "   [120.49002   106.36757   101.56152  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[219.71436   204.29085   174.       ]\n",
            "   [221.30109   210.14293   180.04597  ]\n",
            "   [219.18884   210.8317    179.52557  ]\n",
            "   ...\n",
            "   [164.7346    128.28558   101.33149  ]\n",
            "   [136.08669    94.57135    69.38764  ]\n",
            "   [127.24425    84.66778    58.88704  ]]\n",
            "\n",
            "  [[220.35715   203.31123   169.49997  ]\n",
            "   [222.00002   211.7143    176.63776  ]\n",
            "   [218.        212.30103   180.7143   ]\n",
            "   ...\n",
            "   [149.46936   109.556046   74.77025  ]\n",
            "   [146.76508   101.6273     68.41297  ]\n",
            "   [124.09675    77.953865   44.739536 ]]\n",
            "\n",
            "  [[220.28561   204.21419   164.85696  ]\n",
            "   [223.21422   212.9285    176.85707  ]\n",
            "   [218.28564   213.71422   181.9285   ]\n",
            "   ...\n",
            "   [137.09186    95.454      53.515205 ]\n",
            "   [153.46426   107.61214    66.30089  ]\n",
            "   [128.90785    79.90786    39.550682 ]]]\n",
            "\n",
            "\n",
            " [[[138.4847    124.22959   124.35714  ]\n",
            "   [137.55103   125.474495  127.50001  ]\n",
            "   [141.64285   130.64285   137.07143  ]\n",
            "   ...\n",
            "   [136.80623    76.66337    71.091934 ]\n",
            "   [123.571205   77.26518    64.8825   ]\n",
            "   [ 86.28536    57.571255   39.28547  ]]\n",
            "\n",
            "  [[133.16327   116.30612   116.33163  ]\n",
            "   [133.0051    117.85204   120.71429  ]\n",
            "   [135.81633   123.244896  130.45918  ]\n",
            "   ...\n",
            "   [ 91.51526    53.086735   56.51528  ]\n",
            "   [ 79.68354    53.035667   50.31115  ]\n",
            "   [ 54.265      43.311054   35.311016 ]]\n",
            "\n",
            "  [[142.2143    121.92857   123.07143  ]\n",
            "   [140.39795   124.18367   127.11224  ]\n",
            "   [140.92856   128.18878   135.73979  ]\n",
            "   ...\n",
            "   [ 58.336636   42.168327   50.739735 ]\n",
            "   [ 54.566338   46.739864   50.40821  ]\n",
            "   [ 51.20389    51.13764    50.137608 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 74.137764   39.994884   20.857126 ]\n",
            "   [ 68.22959    35.19898    16.198978 ]\n",
            "   [ 64.         33.428574   15.857143 ]\n",
            "   ...\n",
            "   [167.76532   139.55106   103.19385  ]\n",
            "   [169.75511   143.75511   106.75512  ]\n",
            "   [164.99487   139.20917   101.566284 ]]\n",
            "\n",
            "  [[ 76.102066   39.193882   20.147976 ]\n",
            "   [ 74.653175   39.653145   20.719473 ]\n",
            "   [ 69.704185   39.13276    21.561333 ]\n",
            "   ...\n",
            "   [171.64291   143.42865   107.07144  ]\n",
            "   [169.99991   143.99991   106.99991  ]\n",
            "   [164.49489   139.49489    99.49489  ]]\n",
            "\n",
            "  [[ 78.785706   41.0153     22.0153   ]\n",
            "   [ 70.04084    34.897987   15.969416 ]\n",
            "   [ 75.69387    44.846935   26.846937 ]\n",
            "   ...\n",
            "   [164.7143    136.50003   100.14282  ]\n",
            "   [155.66327   130.66327    90.66326  ]\n",
            "   [159.4747    134.4747     94.47469  ]]]\n",
            "\n",
            "\n",
            " [[[ 92.57143    88.571434   79.31632  ]\n",
            "   [102.06633   100.21429    75.87245  ]\n",
            "   [123.59693   124.678566   90.60714  ]\n",
            "   ...\n",
            "   [ 97.         91.         93.       ]\n",
            "   [ 96.07141    90.07141    92.07141  ]\n",
            "   [ 96.668365   88.35715    91.127556 ]]\n",
            "\n",
            "  [[ 75.97449    70.90306    50.326527 ]\n",
            "   [ 80.561226   74.71429    60.612244 ]\n",
            "   [ 87.285706   82.831635   71.31633  ]\n",
            "   ...\n",
            "   [ 97.67346    92.71428    96.5153   ]\n",
            "   [ 97.86226    91.92857    95.78571  ]\n",
            "   [ 97.26021    91.26021    95.117355 ]]\n",
            "\n",
            "  [[ 75.02551    64.811226   55.362247 ]\n",
            "   [ 80.36224    70.93367    64.65816  ]\n",
            "   [ 79.38265    69.67347    66.14285  ]\n",
            "   ...\n",
            "   [ 99.50511    96.99999   100.59694  ]\n",
            "   [100.9694     96.015305  100.42857  ]\n",
            "   [101.         96.        100.42857  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 14.423368   13.423368   11.423368 ]\n",
            "   [ 20.52547    19.52547    17.52547  ]\n",
            "   [ 23.188814   22.188814   20.188814 ]\n",
            "   ...\n",
            "   [ 13.785767   15.785767   14.357239 ]\n",
            "   [ 11.984669   16.04081    14.413319 ]\n",
            "   [  9.571411   13.785675   12.142883 ]]\n",
            "\n",
            "  [[  9.239787    9.239787    7.2397876]\n",
            "   [  6.2245307   6.2245307   4.2245307]\n",
            "   [  7.7602386   7.7602386   5.7602386]\n",
            "   ...\n",
            "   [ 15.158162   17.086721   16.301046 ]\n",
            "   [ 14.142822   17.994898   17.142883 ]\n",
            "   [ 11.760221   15.760221   14.903104 ]]\n",
            "\n",
            "  [[ 12.301147   14.301147   11.301147 ]\n",
            "   [ 13.903129   15.903129   12.903129 ]\n",
            "   [ 15.066389   17.06639    14.066389 ]\n",
            "   ...\n",
            "   [ 13.1377535  14.1377535  16.137753 ]\n",
            "   [ 12.714233   16.5        17.571411 ]\n",
            "   [ 11.642822   15.642822   16.642822 ]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pq4RaKQPBjNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXnLvoWeBjMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsqN-AvNBi-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjd-oyDkBi1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkyJEcj5BiZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NqQAxoF8G2Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnBdBbGiG2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUlwFUZuG2Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7RDuUHdG1--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39lSUkHzG12c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Fj0SvaaG1tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGMBiOneG1kM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}