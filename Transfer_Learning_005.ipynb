{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T06:47:23.876758Z",
          "iopub.execute_input": "2025-09-17T06:47:23.877071Z",
          "iopub.status.idle": "2025-09-17T06:47:23.890789Z",
          "shell.execute_reply.started": "2025-09-17T06:47:23.877040Z",
          "shell.execute_reply": "2025-09-17T06:47:23.887926Z"
        },
        "id": "It3-4Qw_cBQ6"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning with TensorFlow: Feature Extraction**\n",
        "\n",
        "To improve models we can use several tenchniques like\n",
        "- Adding More Layers\n",
        "- Changing the Learning Rate\n",
        "- Adjusting the Number of Neurons Per Layer\n",
        "\n",
        "However, instead of above we can use **Transfer Learning**.\n",
        "- **Transfer Learning** is taking the patterns (also called weights) from another model and applying it on new problem.\n",
        "\n",
        "Two main benefits of using Transfer Learning.\n",
        "- Leverage existing Neural Network Architecture proven to work on problems similar to our own\n",
        "- Using **Already Learned Patterns** on similar data to our own\n",
        "\n",
        "So, instead of hand-crafting our own **Neural Network Architecture or building them from scratch** we can utilize models which have worked for others.\n",
        "\n",
        "We can take the patterns a model has learned from datasets such as **ImageNet** and use it as a foundational model.\n",
        "\n",
        "**What we will Learn**\n",
        "- Use a smaller dataset to experiment faster (10% of training samples of 10 classes of food)\n",
        "- Build a Transfer Learning Feature Extraction model using **TensorFlow Hub**\n",
        "- Introduce a TesnorBoard Callback to track model training results\n",
        "\n",
        "**Transfer Learning with TensorFlow Hub: Getting great results with only 10% of data**\n",
        "\n",
        "- **TensorFlow Hub:-** is a repository for existing model components. You can import and use a **Fully Trained Model** using a *URL*\n",
        "\n",
        "Using the **Pre-trained Models** we can get the results of a fully trained model with only 10% of data.\n",
        "\n",
        "**Transfer Learning often allows you to get great results with less data**\n",
        "\n",
        "\n",
        "Let's download 10% of training data from **10_food_classes** dataset and use it to train a food image classifier on it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n919sal9cUXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VyldLoxcStk",
        "outputId": "a5481209-4dcc-45c2-de5a-fcd72b087b0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 08:34:59--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.137.207, 142.250.101.207, 142.251.2.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.137.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   213MB/s    in 0.8s    \n",
            "\n",
            "2025-09-22 08:35:00 (213 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "BB5IpZFIleoc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hccRg2Vledy",
        "outputId": "fe280a11-44b8-4932-b7a4-a9d1b5f304fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Data Loaders (Preparing the Data)**\n",
        "\n",
        "- Create the **ImageDataGenerator** class using the **flow_from_directory** method to load in our images."
      ],
      "metadata": {
        "id": "2vwq043HneRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "print(\"Training Images\")\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                                          target_size=IMAGE_SHAPE,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode=\"categorical\"\n",
        "                                                          )\n",
        "\n",
        "\n",
        "print(\"Testing Images\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=IMAGE_SHAPE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             class_mode=\"categorical\"\n",
        "                                             )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHc9cw-8leaS",
        "outputId": "7e050b06-2cd6-468e-e995-e42dc63288a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Images\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing Images\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Callbacks (Things to Run While our Model Trains)**\n",
        "\n",
        "**Callbacks** are extra functionality that you can add to your models to be performed during or after training. Some of the most important callbacks are\n",
        "\n",
        "- **Experiment Tracking with TensorBoard:-** Log the performance of multiple models and then view and compare these models in a visual way on **TensorBoard**. **TensorBoard** is a dashboard for inspecting **Neural Network Parameters**\n",
        "\n",
        "- **Model CheckPointing:-** Save your model as you train so that you can stop training if needed and continue off where you left. It is helpful if training takes a long time and cannot be done in one sitting.\n",
        "\n",
        "- **Early Stopping:-** Leave your model training for a arbitary amount of time and have it stop training automatically when it ceases to improve. It is helpful when you have a large dataset and do not know how long training will take.\n",
        "\n",
        "- The TensorBoard Callback can be accessed using\n",
        "**tf.keras.callbacks.TensorBoard()**. - The main function of this is saving model's training performance metrics to a specified **log_dir**.\n",
        "\n",
        "- By default, logs are recorded every epoch using the **update_freq='epoch'** parameter. This is a good default but can slow down **Model Training.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wc7otuv_pQRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tensorboard callback\n",
        "\n",
        "import datetime\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n"
      ],
      "metadata": {
        "id": "lt9_L9wHleQ7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will save the **Model** to a directory [dir_name] / [experiment_name] / [current_timestamp] where\n",
        "- **dir_name:-** is the overall logs directory\n",
        "- **experiment_name:-** is the particular experiment\n",
        "- **current_timestamp:-** is the time the experiment started based on Python time *datatime.datetime().now()"
      ],
      "metadata": {
        "id": "M1wTBkSs2VWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Models Using TensorFlow Hub**\n",
        "\n",
        "- In past we used to create models from scratch\n",
        "- In here, majority of our model's layers are going to come from **TensorFlow Hub**\n",
        "\n",
        "We will use two models from **TensorFlow Hub**\n",
        "\n",
        "- 1.**ResNetV2** - a state of the art computer vision model architecture from 2016\n",
        "- 2.**EfficientNet**- a state of the art computer vision model from 2019\n",
        "\n",
        "By the state of art we mean that majority of our modeles have achieved the lowest error rate on **ImageNet (ILSVRC-2012-CLS)**, the gold standard of computer vision benchmarks.\n",
        "\n",
        "Steps for finding models on **TensorFlow Hub**\n",
        "1. Got to **tfhub.dev**\n",
        "2. Select the problem domain like **Image**\n",
        "3. Remove all **Problem Domain** filters except for the one you are working on\n",
        "4. You will see a list of models, select the one you want to use\n",
        "\n",
        "**I see many models, then which one is to be used**\n",
        "\n",
        "- You can find a list of state of the art models on **paperswithcode.com**, a resource for collecting the latest in deep learning paper results\n",
        "\n",
        "- Since, our target is **Image Classification** so we would use the model that performed best on **ImageNet.**\n",
        "\n",
        "- On **tfhub.dev** you will find various architectures like **EfficientNetB4** which is better than **EfficientNetB0** but larger models take alot of time to compute.\n"
      ],
      "metadata": {
        "id": "zWUKJMzLZ58L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use feature vectors URLs of two common computer vision architectures, **EfficientNetBO (2019)** and **ResNetV250 (2016)**\n",
        "\n",
        "Why we select only **Feature Vectors**.\n",
        "Because Transfer Learning come into play as **Feature Extraction** and **Fine Tuning**\n",
        "\n",
        "1. In **transfer Learning** we take a pre-trained models as it is and apply it to our task without changes. For example, if your model is trained on **ImageNet** dataset that contains **1000** different classes of images. So, if we pass a single image to this model it will produce **1000** different outputs. It can be useful if we want to classify **1000** images.\n",
        "\n",
        "2. **Feature Extraction Transfer Learning** is a process where you take the underlying patterns (also called weights) a **pretrained Model** has learned and adjust its output to be more suited to your problem.\n",
        "For example, If your model had 236 different layers (EfficientNetBO has 236 layers) and the top layer outputs **1000** classes because it was pretrained on **ImageNet.**To adjust it to your problem we might remove the top layer and replace it with our own having the right number of classes. The most important part here is that **only the top few layers become trainable, the rest remain frozen.**So, the underlying patterns remain in the rest of layers and we can utilize it for our problem.\n",
        "\n",
        "3. **Fine-Tuning Transfer Learning:-** is when you take the underlying patterns (also called weights) of a pre-trained model and adjust them to your problem. This means **training some, many or all layers** in pretrained model. This is applicable in scenarios where you have relatively large dataset and **your data is slightly different** from the original data on which the model was trained.\n",
        "\n",
        "\n",
        "A common practice is to **Freeze** all the learned patterns in bottom layers of a **pretrained model** so that they become **un-trainable**. Then, the top 2-3 layers of the **pre-trained** model can adjust its output to our customer data (**feature extraction**).\n",
        "As you have trained the **top layers** you can gradually **unfreeze** more and more layers and run the training process on your own data to further **fine-tune**it.\n",
        "\n",
        "- **Lower Layers** in a computer vision model learns **large features**. In a cat and dog classification they might learn the **outline of legs** while the layers closer to the output might learn **shape of the teeth.**\n",
        "\n",
        "\n",
        "So, in **Feature Extraction** only the top 2-3 layers change but in **Fine Tuning Model** many or all of the original model get changed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nc2Q_27koE2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "cfuo9Vl8leIA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet 50 V2 feature vector\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Original: EfficientNetB0 feature vector (version 1)\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "\n",
        "# # New: EfficientNetB0 feature vector (version 2)\n",
        "# efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n"
      ],
      "metadata": {
        "id": "98_y2fW9ld_Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **URLs** are link to a saved **pretrained models** on **TensorFlow Hub**.\n",
        "- When we use it in our model, the model will automatically downloaded for us to use\n",
        "- We will use **KerasLayer()** inside the TensorFlow Hub library."
      ],
      "metadata": {
        "id": "Vpry_f-YzbJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The function below helps in creating **Model**.\n",
        "- Our first model will be **ResNetV250** architecture as our feature extraction layer\n",
        "- Once our model is instantiated, we will compile it using **categorical_crossentropy** as our loss function, **Adam Optimizer** and **Accuracy** as metric."
      ],
      "metadata": {
        "id": "FY2i_Kc31LC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "NUM_CLASSES = 10  # Replace with the actual number of classes in your dataset\n",
        "RESNET_URL = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Custom layer to wrap hub.KerasLayer\n",
        "class HubLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_url, **kwargs):\n",
        "        super(HubLayer, self).__init__(**kwargs)\n",
        "        self.hub_layer = hub.KerasLayer(model_url, trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.hub_layer(inputs)\n",
        "\n",
        "def create_model_tf_hub_fixed(model_url, num_classes=NUM_CLASSES):\n",
        "    # Define input layer\n",
        "    inputs = keras.Input(shape=IMAGE_SHAPE + (3,))\n",
        "\n",
        "    # Apply custom Hub layer\n",
        "    x = HubLayer(model_url)(inputs)\n",
        "\n",
        "    # Add dropout and dense layers\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Clear Keras session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Disable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "# Create and compile the model\n",
        "resnet_model = create_model_tf_hub_fixed(RESNET_URL, num_classes=NUM_CLASSES)\n",
        "resnet_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "resnet_model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XOZekZ2woAIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "be972208-7504-4d89-c406-643a21714c82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hub_layer (\u001b[38;5;33mHubLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m20,490\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hub_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">HubLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,490\u001b[0m (80.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> (80.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,490\u001b[0m (80.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> (80.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding the CallBack**\n",
        "- A tensorflow **CallBack** adds extra functionality by virtue of which we can track the performance of our model on TensorBoard\n",
        "- The **CallBack** is added in the **fit** method"
      ],
      "metadata": {
        "id": "5xZTf5MbhZDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_history = resnet_model.fit(\n",
        "    train_data_10_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data),\n",
        "    callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                           experiment_name=\"resnet50V2\")]\n",
        ")\n"
      ],
      "metadata": {
        "id": "XSdlwwNMoAE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d8f01e-b2d4-48a9-81e1-924847d4f5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: tensorflow_hub/resnet50V2/20250922-084601\n",
            "Epoch 1/5\n",
            "\u001b[1m 1/24\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:15\u001b[0m 61s/step - accuracy: 0.0625 - loss: 3.2295"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wignj0fUn_9M"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}