{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-17T06:47:23.876758Z",
          "iopub.execute_input": "2025-09-17T06:47:23.877071Z",
          "iopub.status.idle": "2025-09-17T06:47:23.890789Z",
          "shell.execute_reply.started": "2025-09-17T06:47:23.877040Z",
          "shell.execute_reply": "2025-09-17T06:47:23.887926Z"
        },
        "id": "It3-4Qw_cBQ6"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning with TensorFlow: Feature Extraction**\n",
        "\n",
        "To improve models we can use several tenchniques like\n",
        "- Adding More Layers\n",
        "- Changing the Learning Rate\n",
        "- Adjusting the Number of Neurons Per Layer\n",
        "\n",
        "However, instead of above we can use **Transfer Learning**.\n",
        "- **Transfer Learning** is taking the patterns (also called weights) from another model and applying it on new problem.\n",
        "\n",
        "Two main benefits of using Transfer Learning.\n",
        "- Leverage existing Neural Network Architecture proven to work on problems similar to our own\n",
        "- Using **Already Learned Patterns** on similar data to our own\n",
        "\n",
        "So, instead of hand-crafting our own **Neural Network Architecture or building them from scratch** we can utilize models which have worked for others.\n",
        "\n",
        "We can take the patterns a model has learned from datasets such as **ImageNet** and use it as a foundational model.\n",
        "\n",
        "**What we will Learn**\n",
        "- Use a smaller dataset to experiment faster (10% of training samples of 10 classes of food)\n",
        "- Build a Transfer Learning Feature Extraction model using **TensorFlow Hub**\n",
        "- Introduce a TesnorBoard Callback to track model training results\n",
        "\n",
        "**Transfer Learning with TensorFlow Hub: Getting great results with only 10% of data**\n",
        "\n",
        "- **TensorFlow Hub:-** is a repository for existing model components. You can import and use a **Fully Trained Model** using a *URL*\n",
        "\n",
        "Using the **Pre-trained Models** we can get the results of a fully trained model with only 10% of data.\n",
        "\n",
        "**Transfer Learning often allows you to get great results with less data**\n",
        "\n",
        "\n",
        "Let's download 10% of training data from **10_food_classes** dataset and use it to train a food image classifier on it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n919sal9cUXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VyldLoxcStk",
        "outputId": "b9a2610d-3493-4672-9d7b-8374fd1d2bb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-23 09:30:33--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 74.125.137.207, 142.250.101.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   181MB/s    in 0.9s    \n",
            "\n",
            "2025-09-23 09:30:34 (181 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "BB5IpZFIleoc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hccRg2Vledy",
        "outputId": "9e5c2b6d-df90-4d4d-e5ac-fdd90aeadbf1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Data Loaders (Preparing the Data)**\n",
        "\n",
        "- Create the **ImageDataGenerator** class using the **flow_from_directory** method to load in our images."
      ],
      "metadata": {
        "id": "2vwq043HneRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "print(\"Training Images\")\n",
        "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
        "                                                          target_size=IMAGE_SHAPE,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          class_mode=\"categorical\"\n",
        "                                                          )\n",
        "\n",
        "\n",
        "print(\"Testing Images\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=IMAGE_SHAPE,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             class_mode=\"categorical\"\n",
        "                                             )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHc9cw-8leaS",
        "outputId": "159fac75-4246-4581-b164-292ca0918187"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Images\n",
            "Found 750 images belonging to 10 classes.\n",
            "Testing Images\n",
            "Found 2500 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Callbacks (Things to Run While our Model Trains)**\n",
        "\n",
        "**Callbacks** are extra functionality that you can add to your models to be performed during or after training. Some of the most important callbacks are\n",
        "\n",
        "- **Experiment Tracking with TensorBoard:-** Log the performance of multiple models and then view and compare these models in a visual way on **TensorBoard**. **TensorBoard** is a dashboard for inspecting **Neural Network Parameters**\n",
        "\n",
        "- **Model CheckPointing:-** Save your model as you train so that you can stop training if needed and continue off where you left. It is helpful if training takes a long time and cannot be done in one sitting.\n",
        "\n",
        "- **Early Stopping:-** Leave your model training for a arbitary amount of time and have it stop training automatically when it ceases to improve. It is helpful when you have a large dataset and do not know how long training will take.\n",
        "\n",
        "- The TensorBoard Callback can be accessed using\n",
        "**tf.keras.callbacks.TensorBoard()**. - The main function of this is saving model's training performance metrics to a specified **log_dir**.\n",
        "\n",
        "- By default, logs are recorded every epoch using the **update_freq='epoch'** parameter. This is a good default but can slow down **Model Training.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wc7otuv_pQRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tensorboard callback\n",
        "\n",
        "import datetime\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n"
      ],
      "metadata": {
        "id": "lt9_L9wHleQ7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We will save the **Model** to a directory [dir_name] / [experiment_name] / [current_timestamp] where\n",
        "- **dir_name:-** is the overall logs directory\n",
        "- **experiment_name:-** is the particular experiment\n",
        "- **current_timestamp:-** is the time the experiment started based on Python time *datatime.datetime().now()"
      ],
      "metadata": {
        "id": "M1wTBkSs2VWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Models Using TensorFlow Hub**\n",
        "\n",
        "- In past we used to create models from scratch\n",
        "- In here, majority of our model's layers are going to come from **TensorFlow Hub**\n",
        "\n",
        "We will use two models from **TensorFlow Hub**\n",
        "\n",
        "- 1.**ResNetV2** - a state of the art computer vision model architecture from 2016\n",
        "- 2.**EfficientNet**- a state of the art computer vision model from 2019\n",
        "\n",
        "By the state of art we mean that majority of our modeles have achieved the lowest error rate on **ImageNet (ILSVRC-2012-CLS)**, the gold standard of computer vision benchmarks.\n",
        "\n",
        "Steps for finding models on **TensorFlow Hub**\n",
        "1. Got to **tfhub.dev**\n",
        "2. Select the problem domain like **Image**\n",
        "3. Remove all **Problem Domain** filters except for the one you are working on\n",
        "4. You will see a list of models, select the one you want to use\n",
        "\n",
        "**I see many models, then which one is to be used**\n",
        "\n",
        "- You can find a list of state of the art models on **paperswithcode.com**, a resource for collecting the latest in deep learning paper results\n",
        "\n",
        "- Since, our target is **Image Classification** so we would use the model that performed best on **ImageNet.**\n",
        "\n",
        "- On **tfhub.dev** you will find various architectures like **EfficientNetB4** which is better than **EfficientNetB0** but larger models take alot of time to compute.\n"
      ],
      "metadata": {
        "id": "zWUKJMzLZ58L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use feature vectors URLs of two common computer vision architectures, **EfficientNetBO (2019)** and **ResNetV250 (2016)**\n",
        "\n",
        "Why we select only **Feature Vectors**.\n",
        "Because Transfer Learning come into play as **Feature Extraction** and **Fine Tuning**\n",
        "\n",
        "1. In **transfer Learning** we take a pre-trained models as it is and apply it to our task without changes. For example, if your model is trained on **ImageNet** dataset that contains **1000** different classes of images. So, if we pass a single image to this model it will produce **1000** different outputs. It can be useful if we want to classify **1000** images.\n",
        "\n",
        "2. **Feature Extraction Transfer Learning** is a process where you take the underlying patterns (also called weights) a **pretrained Model** has learned and adjust its output to be more suited to your problem.\n",
        "For example, If your model had 236 different layers (EfficientNetBO has 236 layers) and the top layer outputs **1000** classes because it was pretrained on **ImageNet.**To adjust it to your problem we might remove the top layer and replace it with our own having the right number of classes. The most important part here is that **only the top few layers become trainable, the rest remain frozen.**So, the underlying patterns remain in the rest of layers and we can utilize it for our problem.\n",
        "\n",
        "3. **Fine-Tuning Transfer Learning:-** is when you take the underlying patterns (also called weights) of a pre-trained model and adjust them to your problem. This means **training some, many or all layers** in pretrained model. This is applicable in scenarios where you have relatively large dataset and **your data is slightly different** from the original data on which the model was trained.\n",
        "\n",
        "\n",
        "A common practice is to **Freeze** all the learned patterns in bottom layers of a **pretrained model** so that they become **un-trainable**. Then, the top 2-3 layers of the **pre-trained** model can adjust its output to our customer data (**feature extraction**).\n",
        "As you have trained the **top layers** you can gradually **unfreeze** more and more layers and run the training process on your own data to further **fine-tune**it.\n",
        "\n",
        "- **Lower Layers** in a computer vision model learns **large features**. In a cat and dog classification they might learn the **outline of legs** while the layers closer to the output might learn **shape of the teeth.**\n",
        "\n",
        "\n",
        "So, in **Feature Extraction** only the top 2-3 layers change but in **Fine Tuning Model** many or all of the original model get changed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nc2Q_27koE2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "cfuo9Vl8leIA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnet 50 V2 feature vector\n",
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Original: EfficientNetB0 feature vector (version 1)\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "\n",
        "# # New: EfficientNetB0 feature vector (version 2)\n",
        "# efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n"
      ],
      "metadata": {
        "id": "98_y2fW9ld_Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **URLs** are link to a saved **pretrained models** on **TensorFlow Hub**.\n",
        "- When we use it in our model, the model will automatically downloaded for us to use\n",
        "- We will use **KerasLayer()** inside the TensorFlow Hub library."
      ],
      "metadata": {
        "id": "Vpry_f-YzbJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The function below helps in creating **Model**.\n",
        "- Our first model will be **ResNetV250** architecture as our feature extraction layer\n",
        "- Once our model is instantiated, we will compile it using **categorical_crossentropy** as our loss function, **Adam Optimizer** and **Accuracy** as metric."
      ],
      "metadata": {
        "id": "FY2i_Kc31LC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "NUM_CLASSES = 10  # Replace with the actual number of classes in your dataset\n",
        "RESNET_URL = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
        "\n",
        "# Custom layer to wrap hub.KerasLayer\n",
        "class HubLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, model_url, **kwargs):\n",
        "        super(HubLayer, self).__init__(**kwargs)\n",
        "        self.hub_layer = hub.KerasLayer(model_url, trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.hub_layer(inputs)\n",
        "\n",
        "def create_model_tf_hub_fixed(model_url, num_classes=NUM_CLASSES):\n",
        "    # Define input layer\n",
        "    inputs = keras.Input(shape=IMAGE_SHAPE + (3,))\n",
        "\n",
        "    # Apply custom Hub layer\n",
        "    x = HubLayer(model_url)(inputs)\n",
        "\n",
        "    # Add dropout and dense layers\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Clear Keras session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Disable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "# Create and compile the model\n",
        "resnet_model = create_model_tf_hub_fixed(RESNET_URL, num_classes=NUM_CLASSES)\n",
        "resnet_model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "resnet_model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XOZekZ2woAIV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "3e7f90c6-107e-4f8f-e6cb-14ee51e07563"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hub_layer (\u001b[38;5;33mHubLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m20,490\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hub_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">HubLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,490\u001b[0m (80.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> (80.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,490\u001b[0m (80.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> (80.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding the CallBack**\n",
        "- A tensorflow **CallBack** adds extra functionality by virtue of which we can track the performance of our model on TensorBoard\n",
        "- The **CallBack** is added in the **fit** method"
      ],
      "metadata": {
        "id": "5xZTf5MbhZDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_history = resnet_model.fit(\n",
        "    train_data_10_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    validation_data=test_data,\n",
        "    validation_steps=len(test_data),\n",
        "    callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
        "                                           experiment_name=\"resnet50V2\")]\n",
        ")\n"
      ],
      "metadata": {
        "id": "XSdlwwNMoAE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer Learning with TensorFlow Part 2: Fine-Tuning**\n",
        "\n",
        "- In the section above we saw how could we leverage **Feature Extraction Transfer Learning** to get far better results than building our own models\n",
        "- In **Fine Tuning Transfer Learning** the pre-trained weights from another model are **unfrozen and tweaked** during training to better suit your own data\n",
        "- In the **Feature Extraction Transfer Learning** we train the top **1-3 layers** of a pre-trained model with your own data\n",
        "- In the **Fine-tuning transfer learning** we might train 1 to 3 or more layers of a pre-trained model\n",
        "\n",
        "**Main Difference between Feature Extraction Transfer Learning and Fine Tuning Transfer Learning**\n",
        "- **Feature Extraction Transfer Learning:** You only train only top layers\n",
        "- **Fine Tuning Transfer Learning:** Not only top layers but some other layers beneath can be unfrozen for training."
      ],
      "metadata": {
        "id": "nCLAVDjoq9D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Path Ahead**\n",
        "\n",
        "- Introducing Fine-Tuning: A type of transfer learning to modify a pre-trained model to more suited to your data\n",
        "- Using **Keras Functional API**, a different way to build models in keras\n",
        "- Using smaller dataset to experiment faster\n",
        "- **Data Augmentation:** How to making training diverse without adding any new data"
      ],
      "metadata": {
        "id": "No_HgZd_tV60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "bdlS7A9K_Gtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### We create a bunch of helpful functions throughout the course.\n",
        "### Storing them here so they're easily accessible.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
        "  \"\"\"\n",
        "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
        "  (224, 224, 3).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename (str): string filename of target image\n",
        "  img_shape (int): size to resize target image to, default 224\n",
        "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
        "  \"\"\"\n",
        "  # Read in the image\n",
        "  img = tf.io.read_file(filename)\n",
        "  # Decode it into a tensor\n",
        "  img = tf.image.decode_jpeg(img)\n",
        "  # Resize the image\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "  if scale:\n",
        "    # Rescale the image (get all values between 0 and 1)\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img\n",
        "\n",
        "# Note: The following confusion matrix code is a remix of Scikit-Learn's\n",
        "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
        "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):\n",
        "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
        "\n",
        "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
        "  will be used.\n",
        "\n",
        "  Args:\n",
        "    y_true: Array of truth labels (must be same shape as y_pred).\n",
        "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
        "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
        "    figsize: Size of output figure (default=(10, 10)).\n",
        "    text_size: Size of output figure text (default=15).\n",
        "    norm: normalize values or not (default=False).\n",
        "    savefig: save confusion matrix to file (default=False).\n",
        "\n",
        "  Returns:\n",
        "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
        "\n",
        "  Example usage:\n",
        "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
        "                          y_pred=y_preds, # predicted labels\n",
        "                          classes=class_names, # array of class label names\n",
        "                          figsize=(15, 15),\n",
        "                          text_size=10)\n",
        "  \"\"\"\n",
        "  # Create the confustion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
        "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
        "\n",
        "  # Plot the figure and make it pretty\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  # Are there a list of classes?\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "\n",
        "  # Label the axes\n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
        "         yticks=np.arange(n_classes),\n",
        "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
        "         yticklabels=labels)\n",
        "\n",
        "  # Make x-axis labels appear on bottom\n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  # Set the threshold for different colors\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  # Plot the text on each cell\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    if norm:\n",
        "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "    else:\n",
        "      plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "\n",
        "  # Save the figure to the current working directory\n",
        "  if savefig:\n",
        "    fig.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "# Make a function to predict on images and plot them (works with multi-class)\n",
        "def pred_and_plot(model, filename, class_names):\n",
        "  \"\"\"\n",
        "  Imports an image located at filename, makes a prediction on it with\n",
        "  a trained model and plots the image with the predicted class as the title.\n",
        "  \"\"\"\n",
        "  # Import the target image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # Make a prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the predicted class\n",
        "  if len(pred[0]) > 1: # check for multi-class\n",
        "    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n",
        "  else:\n",
        "    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n",
        "\n",
        "  # Plot the image and predicted class\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False);\n",
        "\n",
        "import datetime\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instand to store log files.\n",
        "\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "# Plot the validation and training data separately\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "\n",
        "  Args:\n",
        "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
        "  \"\"\"\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();\n",
        "\n",
        "def compare_historys(original_history, new_history, initial_epochs=5):\n",
        "    \"\"\"\n",
        "    Compares two TensorFlow model History objects.\n",
        "\n",
        "    Args:\n",
        "      original_history: History object from original model (before new_history)\n",
        "      new_history: History object from continued model training (after original_history)\n",
        "      initial_epochs: Number of epochs in original_history (new_history plot starts from here)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get original history measurements\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "\n",
        "    # Combine original history with new history\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    # Make plots\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
        "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "# Create function to unzip a zipfile into current working directory\n",
        "# (since we're going to be downloading and unzipping a few files)\n",
        "import zipfile\n",
        "\n",
        "def unzip_data(filename):\n",
        "  \"\"\"\n",
        "  Unzips filename into the current working directory.\n",
        "\n",
        "  Args:\n",
        "    filename (str): a filepath to a target zip folder to be unzipped.\n",
        "  \"\"\"\n",
        "  zip_ref = zipfile.ZipFile(filename, \"r\")\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()\n",
        "\n",
        "# Walk through an image classification directory and find out how many files (images)\n",
        "# are in each subdirectory.\n",
        "import os\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "\n",
        "  Args:\n",
        "    dir_path (str): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "      y_true: true labels in the form of a 1D array\n",
        "      y_pred: predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results\n",
        "\n"
      ],
      "metadata": {
        "id": "Wignj0fUn_9M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10 Food Classes: Working with Less Data**\n",
        "\n",
        "- Below we will use the **in-built pretrained models** within the **tf.keras.applications** module as well as we will **fine-tune** them to our customer dataset.\n",
        "- We will be using a new but similar dataloader function to what we have used before called **image_dataset_from_directory()** which is part of **tf.keras.utils** module.\n",
        "- Finally, we will practice **Keras Functional API** for building deep learning models.\n",
        "- The **Functional API** is a more flexible way to create models than **tf.keras.Sequential API.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxkarKmf_Z04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the Data**"
      ],
      "metadata": {
        "id": "3vzUtCqSBrWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of the data of the 10 classes\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXSAxVg3q8m2",
        "outputId": "6f1f6267-0d32-4a7a-e3c9-eba042e34706"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-24 05:51:44--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.157.207, 142.251.8.207, 142.251.170.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.157.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M  29.2MB/s    in 6.3s    \n",
            "\n",
            "2025-09-24 05:51:50 (25.4 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Walk through 10 percent data directory and list number of files\n",
        "walk_through_dir(\"10_food_classes_10_percent\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5b-DScbq8fe",
        "outputId": "81cc6ad6-a845-42c3-bc7d-ae81dd6cb0e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **training data** has **75 images** while the testing directories have **250 images.**"
      ],
      "metadata": {
        "id": "ifNeR_tiCLtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test directories\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "KYdIoEFvq8cg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Previously we used the **ImageDataGenerator** class but since **August 2023** it has been deprecated and is not recommended for future usage because of its **slowness.**\n",
        "\n",
        "- We will move onto using **tf.keras.utils.image_dataset_from_directory()**\n",
        "\n",
        "- One main benefit of using **tf.keras.preprocessing.image_dataset_from_directory()** rather than **ImageDataGenerator** is that it creates a **tf.data.Dataset** object rather than a generator\n",
        "\n",
        "- The **tf.data.Dataset** API is much more efficient(faster) than the **ImageDataGenerator** API which is paramount for larger datasets"
      ],
      "metadata": {
        "id": "IsVip88rCa_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224) # define image size\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
        "\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                           image_size=IMG_SIZE,\n",
        "                                                                           label_mode=\"categorical\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv6dN7SZq8ZM",
        "outputId": "5d45b0e3-ba8c-4a59-ed46-0c4cb7a4e653"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Components of **image_dataset_from_directory()** function\n",
        "\n",
        "- **directory:-** the filepath of the target directory we are loading images in from\n",
        "- **image_size:-** target size\n",
        "- **batch_size:-** batch size we are going to load in\n",
        "- **label_mode:-** If only two classes then **binary** otherwise for more than two classes we use **categorical**"
      ],
      "metadata": {
        "id": "Peavu79HEJ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the training data datatype\n",
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNMUnqT-q8R4",
        "outputId": "627d9288-003e-4cca-8a4e-ba95f86646c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train_data_10_percent** attributes\n",
        "\n",
        "- **(TensorSpec(shape=(None, 224, 224, 3)** refers to the tensor shape of our images. *None* is the batch size. 224 is height and 224 is the width. **3** is the color channels (red, green, blue)\n",
        "\n",
        "- **(None, 10):-** refers to the tensor shape of the labels. **None** is the batch size and **10** is the number of possible labels."
      ],
      "metadata": {
        "id": "X0AmbZ79F5P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the class names of our dataset\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGKOLWqGq8FL",
        "outputId": "736db4c3-b355-48b4-f53c-69dac80cdb55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# See an example batch of data\n",
        "for images, labels in train_data_10_percent.take(1):\n",
        "  print(images, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NruIdE8uBjdG",
        "outputId": "58c4a2eb-f349-44e0-b45c-23e06afacb06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[182.07143   148.07143    87.071434 ]\n",
            "   [177.66837   145.66837    86.668365 ]\n",
            "   [178.85713   148.2857     91.50001  ]\n",
            "   ...\n",
            "   [188.85217   158.85217   104.85218  ]\n",
            "   [202.71437   173.71437   117.71438  ]\n",
            "   [217.35732   188.35732   132.35732  ]]\n",
            "\n",
            "  [[181.56631   146.56631    90.42347  ]\n",
            "   [176.93367   144.93367    87.93367  ]\n",
            "   [179.67346   149.10204    93.88776  ]\n",
            "   ...\n",
            "   [194.0409    164.68369   112.35716  ]\n",
            "   [195.15308   165.15308   111.153076 ]\n",
            "   [198.36224   168.36224   114.36225  ]]\n",
            "\n",
            "  [[178.71428   143.5        89.92857  ]\n",
            "   [177.5       144.5        91.841835 ]\n",
            "   [176.04593   144.47449    93.688774 ]\n",
            "   ...\n",
            "   [190.04596   160.3061    110.90306  ]\n",
            "   [193.55615   163.55615   113.44388  ]\n",
            "   [195.70918   165.70918   114.13775  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[165.28575   172.92854   139.50002  ]\n",
            "   [169.41322   176.41322   142.41322  ]\n",
            "   [165.07141   171.40811   135.23976  ]\n",
            "   ...\n",
            "   [210.69392   177.69392   123.73983  ]\n",
            "   [211.29585   179.29585   122.29586  ]\n",
            "   [217.80103   185.80103   128.80103  ]]\n",
            "\n",
            "  [[163.7347    167.92856   136.33163  ]\n",
            "   [170.85715   174.85715   141.85715  ]\n",
            "   [169.69896   174.12753   140.48466  ]\n",
            "   ...\n",
            "   [211.35715   176.35715   122.35715  ]\n",
            "   [215.19902   180.19902   124.19901  ]\n",
            "   [217.07643   182.07643   124.07643  ]]\n",
            "\n",
            "  [[164.51526   165.51526   134.51526  ]\n",
            "   [173.78572   174.78572   142.78572  ]\n",
            "   [167.28067   171.28067   137.8521   ]\n",
            "   ...\n",
            "   [199.21396   164.21396   110.21396  ]\n",
            "   [213.45401   178.45401   122.45402  ]\n",
            "   [214.7249    179.7249    121.72491  ]]]\n",
            "\n",
            "\n",
            " [[[ 47.92857    44.92857    25.928572 ]\n",
            "   [ 50.54592    47.54592    28.54592  ]\n",
            "   [ 50.209183   47.209183   30.209183 ]\n",
            "   ...\n",
            "   [ 64.07652    59.076523   55.076523 ]\n",
            "   [ 60.954075   55.954075   51.954075 ]\n",
            "   [ 61.71432    56.71432    52.71432  ]]\n",
            "\n",
            "  [[ 49.142857   46.142857   29.806122 ]\n",
            "   [ 48.214287   45.214287   28.346937 ]\n",
            "   [ 49.214287   46.214287   31.102041 ]\n",
            "   ...\n",
            "   [ 59.872467   54.872467   50.872467 ]\n",
            "   [ 57.137753   52.137753   48.137753 ]\n",
            "   [ 58.09694    53.09694    49.09694  ]]\n",
            "\n",
            "  [[ 43.137756   39.566326   27.566326 ]\n",
            "   [ 43.413265   39.84184    27.841837 ]\n",
            "   [ 42.168365   38.59694    26.382652 ]\n",
            "   ...\n",
            "   [ 59.357162   54.357162   50.357162 ]\n",
            "   [ 58.515297   53.515297   49.515297 ]\n",
            "   [ 58.571426   53.571426   49.571426 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 40.13774    39.13774    37.13774  ]\n",
            "   [ 41.         40.         38.       ]\n",
            "   [ 40.357185   39.357185   37.357185 ]\n",
            "   ...\n",
            "   [ 87.50015    85.28589    95.85736  ]\n",
            "   [ 82.94417    80.729904   91.30138  ]\n",
            "   [ 82.51551    80.30125    90.87272  ]]\n",
            "\n",
            "  [[ 38.857117   37.857117   35.857117 ]\n",
            "   [ 38.785675   37.785675   35.785675 ]\n",
            "   [ 42.01527    41.01527    39.01527  ]\n",
            "   ...\n",
            "   [ 85.60209    85.60209    97.60209  ]\n",
            "   [ 87.80609    87.80609    99.80609  ]\n",
            "   [ 79.31633    79.31633    91.31633  ]]\n",
            "\n",
            "  [[ 39.0715     38.0715     36.0715   ]\n",
            "   [ 36.428604   35.428604   33.428604 ]\n",
            "   [ 38.78575    37.78575    35.78575  ]\n",
            "   ...\n",
            "   [ 74.785736   76.785736   88.785736 ]\n",
            "   [ 75.040726   77.040726   89.040726 ]\n",
            "   [ 80.015      82.015      94.015    ]]]\n",
            "\n",
            "\n",
            " [[[ 13.744898   12.928572   15.403061 ]\n",
            "   [ 11.020408   11.377551   14.091837 ]\n",
            "   [ 10.913265   12.066326   14.857142 ]\n",
            "   ...\n",
            "   [ 15.642858   18.000065   25.214329 ]\n",
            "   [ 14.331628   17.331629   24.331629 ]\n",
            "   [ 14.71432    17.714321   24.714321 ]]\n",
            "\n",
            "  [[ 39.454086   32.71429    23.12245  ]\n",
            "   [ 31.137756   27.137758   16.709185 ]\n",
            "   [ 27.14286    23.357145   14.714287 ]\n",
            "   ...\n",
            "   [ 15.6989155  18.056124   25.270388 ]\n",
            "   [ 13.994897   16.994898   23.994898 ]\n",
            "   [ 15.285749   18.28575    25.28575  ]]\n",
            "\n",
            "  [[ 57.285713   41.85714    19.785713 ]\n",
            "   [ 59.984695   45.98469    25.714283 ]\n",
            "   [ 60.897957   47.2551     26.969385 ]\n",
            "   ...\n",
            "   [ 14.474442   16.83165    24.045914 ]\n",
            "   [ 16.285728   19.285728   26.285728 ]\n",
            "   [ 16.709175   19.709175   26.709175 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[171.39787   176.39787   180.95912  ]\n",
            "   [175.12242   180.12242   183.92346  ]\n",
            "   [174.77013   179.38748   183.00485  ]\n",
            "   ...\n",
            "   [109.38262   118.122444   86.525566 ]\n",
            "   [105.14282   114.357086   82.714294 ]\n",
            "   [105.92862   115.00515    84.785736 ]]\n",
            "\n",
            "  [[173.9386    178.9386    184.9386   ]\n",
            "   [176.43358   181.43358   187.43358  ]\n",
            "   [171.35715   176.35715   180.35715  ]\n",
            "   ...\n",
            "   [105.64279   114.         85.32653  ]\n",
            "   [102.214325  111.214325   84.214325 ]\n",
            "   [104.90316   113.90316    86.90316  ]]\n",
            "\n",
            "  [[155.49042   159.49042   168.49042  ]\n",
            "   [ 53.46961    58.46961    64.469604 ]\n",
            "   [178.704     183.704     189.27542  ]\n",
            "   ...\n",
            "   [104.21923   112.57644    85.7907   ]\n",
            "   [103.811226  111.811226   87.811226 ]\n",
            "   [108.58678   116.58678    92.58678  ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  0.          0.          0.       ]\n",
            "   [  0.          0.          0.       ]\n",
            "   [  0.          0.          0.       ]\n",
            "   ...\n",
            "   [ 13.         14.          8.       ]\n",
            "   [ 13.         14.          8.       ]\n",
            "   [ 13.         14.          8.       ]]\n",
            "\n",
            "  [[  0.          0.          0.       ]\n",
            "   [  0.          0.          0.       ]\n",
            "   [  0.          0.          0.       ]\n",
            "   ...\n",
            "   [ 13.785706   14.          6.       ]\n",
            "   [ 13.         14.          6.       ]\n",
            "   [ 13.         14.          6.       ]]\n",
            "\n",
            "  [[  0.          0.          0.       ]\n",
            "   [  0.          0.          0.       ]\n",
            "   [  0.          0.          0.       ]\n",
            "   ...\n",
            "   [ 14.428572   13.785714    6.       ]\n",
            "   [ 14.428572   13.785714    6.       ]\n",
            "   [ 14.         14.          6.       ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 93.24378    91.02952    77.60099  ]\n",
            "   [136.26973   132.54013   119.69835  ]\n",
            "   [165.08139   162.08139   146.65286  ]\n",
            "   ...\n",
            "   [ 83.82545    77.82545    63.82551  ]\n",
            "   [ 43.942516   40.728283   25.513988 ]\n",
            "   [ 17.060741   14.285231    3.8925219]]\n",
            "\n",
            "  [[  5.158166    2.1122391   0.       ]\n",
            "   [ 19.474289   16.474289    9.683537 ]\n",
            "   [ 64.290184   60.50447    50.106544 ]\n",
            "   ...\n",
            "   [ 13.433512    9.749867    1.459147 ]\n",
            "   [  4.4029274   4.2703395   0.       ]\n",
            "   [  1.5254847   2.7907789   0.       ]]\n",
            "\n",
            "  [[  5.9743824   1.9743826   0.8724017]\n",
            "   [  6.295759    2.6988525   0.5969064]\n",
            "   [  8.984811    5.984811    1.1225584]\n",
            "   ...\n",
            "   [  3.2857666   1.5562651   0.       ]\n",
            "   [  3.239862    4.0970397   0.       ]\n",
            "   [  0.9438766   2.8979218   0.       ]]]\n",
            "\n",
            "\n",
            " [[[  4.229592    3.2295918   0.       ]\n",
            "   [  5.          4.          0.       ]\n",
            "   [  4.8622446   3.8622448   0.       ]\n",
            "   ...\n",
            "   [137.86227   121.3674     99.85205  ]\n",
            "   [169.12244   155.12244   127.97955  ]\n",
            "   [162.53065   151.53065   119.530655 ]]\n",
            "\n",
            "  [[  3.0459182   2.0459182   0.       ]\n",
            "   [  4.          3.          0.       ]\n",
            "   [  3.8010201   2.8010201   0.       ]\n",
            "   ...\n",
            "   [175.96443   159.94913   135.53581  ]\n",
            "   [172.55588   157.75998   128.69362  ]\n",
            "   [152.73494   139.73494   107.73493  ]]\n",
            "\n",
            "  [[  3.7193878   2.7193878   0.       ]\n",
            "   [  4.214286    3.2142859   0.       ]\n",
            "   [  4.1683674   3.1683674   0.       ]\n",
            "   ...\n",
            "   [167.76512   150.12225   122.67325  ]\n",
            "   [148.46944   131.59702   101.38268  ]\n",
            "   [154.4285    139.21422   106.13768  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[189.        173.50511   136.1428   ]\n",
            "   [189.        173.72961   135.57138  ]\n",
            "   [188.78572   173.83165   135.8571   ]\n",
            "   ...\n",
            "   [190.        176.        137.       ]\n",
            "   [191.85712   178.85712   136.85712  ]\n",
            "   [190.63783   177.63783   133.63783  ]]\n",
            "\n",
            "  [[187.95407   172.95407   131.95407  ]\n",
            "   [186.93365   171.93365   130.93365  ]\n",
            "   [186.9847    172.41327   131.19897  ]\n",
            "   ...\n",
            "   [189.07144   175.07144   136.07144  ]\n",
            "   [191.92346   178.92346   136.92346  ]\n",
            "   [191.64291   178.64291   134.64291  ]]\n",
            "\n",
            "  [[188.58676   173.58676   130.58676  ]\n",
            "   [186.40309   171.40309   128.40309  ]\n",
            "   [187.07654   172.50511   129.29082  ]\n",
            "   ...\n",
            "   [190.35718   176.35718   137.35718  ]\n",
            "   [191.8316    178.8316    136.8316   ]\n",
            "   [190.12758   177.12758   133.12758  ]]]\n",
            "\n",
            "\n",
            " [[[111.571434  104.571434   26.571428 ]\n",
            "   [112.59694   105.59694    25.59694  ]\n",
            "   [113.        105.         24.       ]\n",
            "   ...\n",
            "   [ 38.3623     22.076588   15.076589 ]\n",
            "   [ 38.21427    21.928558   14.928558 ]\n",
            "   [ 38.158184   21.87247    14.872472 ]]\n",
            "\n",
            "  [[105.52551    98.38266    20.45408  ]\n",
            "   [105.852036   96.852036   17.994898 ]\n",
            "   [104.984695   96.18368    16.785715 ]\n",
            "   ...\n",
            "   [ 34.67349    19.673489   12.673488 ]\n",
            "   [ 35.92857    20.928572   13.928572 ]\n",
            "   [ 33.474453   18.474453   11.474453 ]]\n",
            "\n",
            "  [[104.28061    95.28061    18.280613 ]\n",
            "   [102.27041    93.27041    16.270409 ]\n",
            "   [101.04592    92.04592    14.617347 ]\n",
            "   ...\n",
            "   [ 32.42855    18.999977   12.785692 ]\n",
            "   [ 34.158154   20.729582   14.515296 ]\n",
            "   [ 30.219292   16.79072    10.576433 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 10.137761   10.137761    8.137761 ]\n",
            "   [ 12.214294   11.214294    7.2142944]\n",
            "   [ 12.642858   11.          7.214286 ]\n",
            "   ...\n",
            "   [171.83162   148.83162    96.40309  ]\n",
            "   [170.72957   147.72957    97.30099  ]\n",
            "   [170.        147.         96.57141  ]]\n",
            "\n",
            "  [[ 11.239807   11.239807    9.239807 ]\n",
            "   [ 13.066327   12.066327    8.066327 ]\n",
            "   [ 11.913252   10.270394    6.48468  ]\n",
            "   ...\n",
            "   [171.0153    148.0153     94.12752  ]\n",
            "   [170.        147.         95.       ]\n",
            "   [169.07141   146.07141    94.07141  ]]\n",
            "\n",
            "  [[ 10.056124   10.056124    8.056124 ]\n",
            "   [ 12.4285755  11.4285755   7.4285755]\n",
            "   [ 10.214291    8.571433    4.785719 ]\n",
            "   ...\n",
            "   [171.        148.13774    93.72452  ]\n",
            "   [169.95407   146.95407    94.95407  ]\n",
            "   [169.        146.         94.       ]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 0: Building a transfer learning model using the Keras Functional API**\n",
        "\n",
        "- The **tf.keras.applications** modules contains a series of already trained **(ImageNet)** computer vision module as well as **Keras Functional API**\n",
        "\n",
        "We are going to go through the following steps\n",
        "\n",
        "- Initialize a pre-trained base model by using a target model such as **EfficientNetV2B0** from **tf.keras.applications.efficientnet_V2** setting **include_top** to **False** to freeze all of the weights because we will build our own top\n",
        "- Set the base model to **trainable** attribute to **False** to freeze all of the weights in the pre-trained model\n",
        "- Define an **input Layer** for our model\n",
        "- **Optional** Normalize the input because some models work better on **Normalized Data** where the input is between 0 and 1.\n",
        "- Pass the input to the **base model.**\n",
        "- Pool the outputs of the **base model** into a shape compatiable with the output activation layer. This turns the **base model output** into same shape as **label tensors.** This can be done using **tf.keras.layers.GlobalAveragePooling2D()** or **tf.keras.layers.GlobalMaxPooling2D** but the former is more common\n",
        "- Create an output **activation** function using **tf.keras.Model()**.\n",
        "- Compile the model using appropriate **loss function** and choose an **optimizer**\n",
        "- Finally, **Fit()** the model using desired number of **epochs** including the desired **callbacks**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C6V-5k-TH3GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the base model\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "# 2. Freeze the base model\n",
        "base_model.trainable = False\n",
        "# 3. Create inputs into the model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3),\n",
        "                               name=\"input_layer\")\n",
        "\n",
        "# 4. If using ResNet50V2, add following to speed up convergence\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
        "\n",
        "'''\n",
        "     5. Pass the inputs to the base_model\n",
        "     (note: using tf.keras.applications,\n",
        "      EfficientNetV2 inputs don't have to be normalized)\n",
        "      '''\n",
        "x = base_model(inputs)\n",
        "# Check data shape after passing it to base_model\n",
        "print(f\"Shape after base_model: {x.shape}\")\n",
        "\n",
        "'''\n",
        "       6. Average pool the outputs of the base model\n",
        "       (aggregate all the most important information,\n",
        "       reduce number of computations)\n",
        " '''\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "print(f\"After GlobalAveragePooling2D(): {x.shape}\")\n",
        "\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10,\n",
        "                                activation=\"softmax\",\n",
        "                                name=\"output_layer\")(x)\n",
        "\n",
        "# 8. Combine the inputs with the outputs into a model\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 9. Compile the model\n",
        "model_0.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq4RaKQPBjNw",
        "outputId": "2cb31abd-adbb-46ff-f8fa-0e29469ff54a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Shape after base_model: (None, 7, 7, 1280)\n",
            "After GlobalAveragePooling2D(): (None, 1280)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Fit the model (we use less steps for validation so it's faster)\n",
        "history_10_percent = model_0.fit(train_data_10_percent,\n",
        "                                 epochs=5,\n",
        "                                 steps_per_epoch=len(train_data_10_percent),\n",
        "                                 validation_data=test_data_10_percent,\n",
        "                                 # Go through less of the validation data so epochs are faster (we want faster experiments!)\n",
        "                                 validation_steps=int(0.25 * len(test_data_10_percent)),\n",
        "                                 # Track our model's training logs for visualization later\n",
        "                                 callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_feature_extract\")])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXnLvoWeBjMd",
        "outputId": "93b5e104-48da-4e0f-cc4a-7649995df549"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: transfer_learning/10_percent_feature_extract/20250924-055358\n",
            "Epoch 1/5\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 3s/step - accuracy: 0.3538 - loss: 2.0320 - val_accuracy: 0.6875 - val_loss: 1.3189\n",
            "Epoch 2/5\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.7456 - loss: 1.2074 - val_accuracy: 0.7878 - val_loss: 0.9052\n",
            "Epoch 3/5\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.8257 - loss: 0.8480 - val_accuracy: 0.8059 - val_loss: 0.7456\n",
            "Epoch 4/5\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.8661 - loss: 0.6900 - val_accuracy: 0.8322 - val_loss: 0.6399\n",
            "Epoch 5/5\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.8819 - loss: 0.5858 - val_accuracy: 0.8487 - val_loss: 0.5825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "- After small training our model performed well on both the **training** and **test** sets\n",
        "- All the good results came because of **Transfer Learning**\n",
        "\n",
        "- In above model what we used is called ** Feature Extraction Transfer Learning**\n",
        "- We passed our customer data to the **already pre-trained model** called **EfficientNetV2B0** to find the patterns that it sees. We then put another layers on top to make our model output our **desired number of classes.**\n",
        "- We also used **Keras Functional API** to build our model rather than the **Sequential API.**"
      ],
      "metadata": {
        "id": "sALVodkoR7hI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's inspect the Layers in our model**"
      ],
      "metadata": {
        "id": "ktu-c52rTAcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check layers in our base model\n",
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number, layer.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsqN-AvNBi-e",
        "outputId": "e9097799-2b05-4824-8a9c-9f68c42d61bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_layer\n",
            "1 rescaling\n",
            "2 normalization\n",
            "3 stem_conv\n",
            "4 stem_bn\n",
            "5 stem_activation\n",
            "6 block1a_project_conv\n",
            "7 block1a_project_bn\n",
            "8 block1a_project_activation\n",
            "9 block2a_expand_conv\n",
            "10 block2a_expand_bn\n",
            "11 block2a_expand_activation\n",
            "12 block2a_project_conv\n",
            "13 block2a_project_bn\n",
            "14 block2b_expand_conv\n",
            "15 block2b_expand_bn\n",
            "16 block2b_expand_activation\n",
            "17 block2b_project_conv\n",
            "18 block2b_project_bn\n",
            "19 block2b_drop\n",
            "20 block2b_add\n",
            "21 block3a_expand_conv\n",
            "22 block3a_expand_bn\n",
            "23 block3a_expand_activation\n",
            "24 block3a_project_conv\n",
            "25 block3a_project_bn\n",
            "26 block3b_expand_conv\n",
            "27 block3b_expand_bn\n",
            "28 block3b_expand_activation\n",
            "29 block3b_project_conv\n",
            "30 block3b_project_bn\n",
            "31 block3b_drop\n",
            "32 block3b_add\n",
            "33 block4a_expand_conv\n",
            "34 block4a_expand_bn\n",
            "35 block4a_expand_activation\n",
            "36 block4a_dwconv2\n",
            "37 block4a_bn\n",
            "38 block4a_activation\n",
            "39 block4a_se_squeeze\n",
            "40 block4a_se_reshape\n",
            "41 block4a_se_reduce\n",
            "42 block4a_se_expand\n",
            "43 block4a_se_excite\n",
            "44 block4a_project_conv\n",
            "45 block4a_project_bn\n",
            "46 block4b_expand_conv\n",
            "47 block4b_expand_bn\n",
            "48 block4b_expand_activation\n",
            "49 block4b_dwconv2\n",
            "50 block4b_bn\n",
            "51 block4b_activation\n",
            "52 block4b_se_squeeze\n",
            "53 block4b_se_reshape\n",
            "54 block4b_se_reduce\n",
            "55 block4b_se_expand\n",
            "56 block4b_se_excite\n",
            "57 block4b_project_conv\n",
            "58 block4b_project_bn\n",
            "59 block4b_drop\n",
            "60 block4b_add\n",
            "61 block4c_expand_conv\n",
            "62 block4c_expand_bn\n",
            "63 block4c_expand_activation\n",
            "64 block4c_dwconv2\n",
            "65 block4c_bn\n",
            "66 block4c_activation\n",
            "67 block4c_se_squeeze\n",
            "68 block4c_se_reshape\n",
            "69 block4c_se_reduce\n",
            "70 block4c_se_expand\n",
            "71 block4c_se_excite\n",
            "72 block4c_project_conv\n",
            "73 block4c_project_bn\n",
            "74 block4c_drop\n",
            "75 block4c_add\n",
            "76 block5a_expand_conv\n",
            "77 block5a_expand_bn\n",
            "78 block5a_expand_activation\n",
            "79 block5a_dwconv2\n",
            "80 block5a_bn\n",
            "81 block5a_activation\n",
            "82 block5a_se_squeeze\n",
            "83 block5a_se_reshape\n",
            "84 block5a_se_reduce\n",
            "85 block5a_se_expand\n",
            "86 block5a_se_excite\n",
            "87 block5a_project_conv\n",
            "88 block5a_project_bn\n",
            "89 block5b_expand_conv\n",
            "90 block5b_expand_bn\n",
            "91 block5b_expand_activation\n",
            "92 block5b_dwconv2\n",
            "93 block5b_bn\n",
            "94 block5b_activation\n",
            "95 block5b_se_squeeze\n",
            "96 block5b_se_reshape\n",
            "97 block5b_se_reduce\n",
            "98 block5b_se_expand\n",
            "99 block5b_se_excite\n",
            "100 block5b_project_conv\n",
            "101 block5b_project_bn\n",
            "102 block5b_drop\n",
            "103 block5b_add\n",
            "104 block5c_expand_conv\n",
            "105 block5c_expand_bn\n",
            "106 block5c_expand_activation\n",
            "107 block5c_dwconv2\n",
            "108 block5c_bn\n",
            "109 block5c_activation\n",
            "110 block5c_se_squeeze\n",
            "111 block5c_se_reshape\n",
            "112 block5c_se_reduce\n",
            "113 block5c_se_expand\n",
            "114 block5c_se_excite\n",
            "115 block5c_project_conv\n",
            "116 block5c_project_bn\n",
            "117 block5c_drop\n",
            "118 block5c_add\n",
            "119 block5d_expand_conv\n",
            "120 block5d_expand_bn\n",
            "121 block5d_expand_activation\n",
            "122 block5d_dwconv2\n",
            "123 block5d_bn\n",
            "124 block5d_activation\n",
            "125 block5d_se_squeeze\n",
            "126 block5d_se_reshape\n",
            "127 block5d_se_reduce\n",
            "128 block5d_se_expand\n",
            "129 block5d_se_excite\n",
            "130 block5d_project_conv\n",
            "131 block5d_project_bn\n",
            "132 block5d_drop\n",
            "133 block5d_add\n",
            "134 block5e_expand_conv\n",
            "135 block5e_expand_bn\n",
            "136 block5e_expand_activation\n",
            "137 block5e_dwconv2\n",
            "138 block5e_bn\n",
            "139 block5e_activation\n",
            "140 block5e_se_squeeze\n",
            "141 block5e_se_reshape\n",
            "142 block5e_se_reduce\n",
            "143 block5e_se_expand\n",
            "144 block5e_se_excite\n",
            "145 block5e_project_conv\n",
            "146 block5e_project_bn\n",
            "147 block5e_drop\n",
            "148 block5e_add\n",
            "149 block6a_expand_conv\n",
            "150 block6a_expand_bn\n",
            "151 block6a_expand_activation\n",
            "152 block6a_dwconv2\n",
            "153 block6a_bn\n",
            "154 block6a_activation\n",
            "155 block6a_se_squeeze\n",
            "156 block6a_se_reshape\n",
            "157 block6a_se_reduce\n",
            "158 block6a_se_expand\n",
            "159 block6a_se_excite\n",
            "160 block6a_project_conv\n",
            "161 block6a_project_bn\n",
            "162 block6b_expand_conv\n",
            "163 block6b_expand_bn\n",
            "164 block6b_expand_activation\n",
            "165 block6b_dwconv2\n",
            "166 block6b_bn\n",
            "167 block6b_activation\n",
            "168 block6b_se_squeeze\n",
            "169 block6b_se_reshape\n",
            "170 block6b_se_reduce\n",
            "171 block6b_se_expand\n",
            "172 block6b_se_excite\n",
            "173 block6b_project_conv\n",
            "174 block6b_project_bn\n",
            "175 block6b_drop\n",
            "176 block6b_add\n",
            "177 block6c_expand_conv\n",
            "178 block6c_expand_bn\n",
            "179 block6c_expand_activation\n",
            "180 block6c_dwconv2\n",
            "181 block6c_bn\n",
            "182 block6c_activation\n",
            "183 block6c_se_squeeze\n",
            "184 block6c_se_reshape\n",
            "185 block6c_se_reduce\n",
            "186 block6c_se_expand\n",
            "187 block6c_se_excite\n",
            "188 block6c_project_conv\n",
            "189 block6c_project_bn\n",
            "190 block6c_drop\n",
            "191 block6c_add\n",
            "192 block6d_expand_conv\n",
            "193 block6d_expand_bn\n",
            "194 block6d_expand_activation\n",
            "195 block6d_dwconv2\n",
            "196 block6d_bn\n",
            "197 block6d_activation\n",
            "198 block6d_se_squeeze\n",
            "199 block6d_se_reshape\n",
            "200 block6d_se_reduce\n",
            "201 block6d_se_expand\n",
            "202 block6d_se_excite\n",
            "203 block6d_project_conv\n",
            "204 block6d_project_bn\n",
            "205 block6d_drop\n",
            "206 block6d_add\n",
            "207 block6e_expand_conv\n",
            "208 block6e_expand_bn\n",
            "209 block6e_expand_activation\n",
            "210 block6e_dwconv2\n",
            "211 block6e_bn\n",
            "212 block6e_activation\n",
            "213 block6e_se_squeeze\n",
            "214 block6e_se_reshape\n",
            "215 block6e_se_reduce\n",
            "216 block6e_se_expand\n",
            "217 block6e_se_excite\n",
            "218 block6e_project_conv\n",
            "219 block6e_project_bn\n",
            "220 block6e_drop\n",
            "221 block6e_add\n",
            "222 block6f_expand_conv\n",
            "223 block6f_expand_bn\n",
            "224 block6f_expand_activation\n",
            "225 block6f_dwconv2\n",
            "226 block6f_bn\n",
            "227 block6f_activation\n",
            "228 block6f_se_squeeze\n",
            "229 block6f_se_reshape\n",
            "230 block6f_se_reduce\n",
            "231 block6f_se_expand\n",
            "232 block6f_se_excite\n",
            "233 block6f_project_conv\n",
            "234 block6f_project_bn\n",
            "235 block6f_drop\n",
            "236 block6f_add\n",
            "237 block6g_expand_conv\n",
            "238 block6g_expand_bn\n",
            "239 block6g_expand_activation\n",
            "240 block6g_dwconv2\n",
            "241 block6g_bn\n",
            "242 block6g_activation\n",
            "243 block6g_se_squeeze\n",
            "244 block6g_se_reshape\n",
            "245 block6g_se_reduce\n",
            "246 block6g_se_expand\n",
            "247 block6g_se_excite\n",
            "248 block6g_project_conv\n",
            "249 block6g_project_bn\n",
            "250 block6g_drop\n",
            "251 block6g_add\n",
            "252 block6h_expand_conv\n",
            "253 block6h_expand_bn\n",
            "254 block6h_expand_activation\n",
            "255 block6h_dwconv2\n",
            "256 block6h_bn\n",
            "257 block6h_activation\n",
            "258 block6h_se_squeeze\n",
            "259 block6h_se_reshape\n",
            "260 block6h_se_reduce\n",
            "261 block6h_se_expand\n",
            "262 block6h_se_excite\n",
            "263 block6h_project_conv\n",
            "264 block6h_project_bn\n",
            "265 block6h_drop\n",
            "266 block6h_add\n",
            "267 top_conv\n",
            "268 top_bn\n",
            "269 top_activation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "bjd-oyDkBi1W",
        "outputId": "7560465c-ad3d-4bb1-8d17-7eb539362779"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b0 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m5,919,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling_layer    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling_layer    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,957,744\u001b[0m (22.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,957,744</span> (22.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,919,312\u001b[0m (22.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> (22.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m25,622\u001b[0m (100.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,622</span> (100.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Summary**\n",
        "\n",
        "- The overall model has 4 layers out of which **efficientnetv2-b0** has 269 layers\n",
        "- Look at the total number of parameters and the **total trainable** parameters. There is a big difference because of **transfer learning**"
      ],
      "metadata": {
        "id": "7WfEvUNEWI6A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkyJEcj5BiZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NqQAxoF8G2Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnBdBbGiG2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUlwFUZuG2Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7RDuUHdG1--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39lSUkHzG12c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Fj0SvaaG1tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGMBiOneG1kM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}